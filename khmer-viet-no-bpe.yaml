# data-no-bpe.yaml

    ## Where the samples will be written
    save_data: data/2025-03-07-16-11/example
    ## Where the vocab(s) will be written
    src_vocab: data/2025-03-07-16-11/example.vocab.src
    tgt_vocab: data/2025-03-07-16-11/example.vocab.tgt
    # Prevent overwriting existing files in the folder
    overwrite: True

    # Corpus opts:
    data:
        corpus_1:
            path_src: data/src-train-token.txt
            path_tgt: data/tgt-train-token.txt
        valid:
            path_src: data/src-val-token.txt
            path_tgt: data/tgt-val-token.txt

    # Vocabulary files that were just created
    src_vocab: models/2025-03-07-16-11/example.vocab.src
    tgt_vocab: models/2025-03-07-16-11/example.vocab.tgt

    # Train on a single GPU
    world_size: 1
    gpu_ranks: [0]

    # Where to save the checkpoints
    save_model: models/2025-03-07-16-11/model
    save_checkpoint_steps: 10000
    train_steps: 130000
    valid_steps: 1000

    # Model
    position_encoding: 'true'
    enc_layers: 6
    dec_layers: 6
    decoder_type: transformer
    encoder_type: transformer
    word_vec_size: 512
    rnn_size: 512
    layers: 6
    transformer_ff: 2048
    heads: 8

    # Batching
    queue_size: 10000
    batch_size: 4096
    valid_batch_size: 4096
    batch_type: tokens
    